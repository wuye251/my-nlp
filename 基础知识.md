# Transformer由来

#### CNN：卷积神经网络

- 处理视觉问题，如图像识别，人脸识别

#### RNN：循环神经网络

- 处理语言任务，如翻译，文本摘要、文本生成等

##### RNN缺点

1. RNN处理需要单词顺序，所以不能通过增加GPU数量来加速训练速度
2. 处理冗长的文本段落很困难，读到结尾时忘记开头发生了什么

### Transformer

Transformer解决了RNN无法并行化的问题， GPT-3， Transformer有三个主要概念：

1. 位置编码  -  即输入的数据集需要是顺序的文本
2. 注意力机制 - 
3. 自注意力机制 - 帮助神经网络消除单词歧义、词性标注、命名实体识别、学习语义角色等

#### BERT

bert基于Transformer实现，已经成为自然语言处理的通用模型，应用场景：

- 文本摘要
- 问答
- 分类
- 命名实体识别
- 文本相似度
- 攻击性信息/脏话检测



